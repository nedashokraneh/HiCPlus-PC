{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "In this Notebook I want to compare learnability of networks by different set of samples (some samples from diagonal frames and some samples from outer frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "model_path = \"/Users/neda/HiCPlus_pytorch/src/models\"\n",
    "sys.path.insert(0, model_path)\n",
    "import model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from time import gmtime, strftime\n",
    "import torch.nn as nn\n",
    "from scipy.stats.stats import pearsonr\n",
    "import argparse\n",
    "use_gpu = 0\n",
    "down_sample_ratio = 16\n",
    "epochs = 50\n",
    "HiC_max_value = 100 # ?????\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.load(\"/Users/neda/HiCPlus_pytorch/data/divided-data/GM12878_primary/10kb_resolution/chr1-17-index.npy\", \"r\")\n",
    "indices = indices.astype(\"int64\")\n",
    "def d_indices(d):\n",
    "    return np.where(indices[:,1] + d == indices[:,2])\n",
    "def corr_highVSlow(index,data1,data2):\n",
    "    return pearsonr(data1[index,0,:,:].flatten(),data2[index,0,:,:].flatten())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 40 0 124.40880613327026 2019-07-08 19:37:21\n",
      "------- 40 1 75.10643544197083 2019-07-08 19:37:23\n",
      "------- 40 2 70.84276552200318 2019-07-08 19:37:25\n",
      "------- 40 3 67.59781122207642 2019-07-08 19:37:27\n",
      "------- 40 4 64.9930793762207 2019-07-08 19:37:29\n",
      "------- 40 5 62.830317687988284 2019-07-08 19:37:32\n",
      "------- 40 6 60.99898591041565 2019-07-08 19:37:34\n",
      "------- 40 7 59.4310221195221 2019-07-08 19:37:36\n",
      "------- 40 8 58.08386068344116 2019-07-08 19:37:38\n",
      "------- 40 9 56.91957378387451 2019-07-08 19:37:40\n",
      "------- 40 10 55.90814266204834 2019-07-08 19:37:43\n",
      "------- 40 11 55.023833465576175 2019-07-08 19:37:45\n",
      "------- 40 12 54.24432578086853 2019-07-08 19:37:47\n",
      "------- 40 13 53.549905729293826 2019-07-08 19:37:49\n",
      "------- 40 14 52.926847505569455 2019-07-08 19:37:52\n",
      "------- 40 15 52.36129431724548 2019-07-08 19:37:54\n",
      "------- 40 16 51.84192886352539 2019-07-08 19:37:56\n",
      "------- 40 17 51.36294407844544 2019-07-08 19:37:59\n",
      "------- 40 18 50.91817355155945 2019-07-08 19:38:03\n",
      "------- 40 19 50.504920196533206 2019-07-08 19:38:06\n",
      "------- 40 20 50.11778845787048 2019-07-08 19:38:09\n",
      "------- 40 21 49.75197582244873 2019-07-08 19:38:11\n",
      "------- 40 22 49.407308959960936 2019-07-08 19:38:14\n",
      "------- 40 23 49.08407297134399 2019-07-08 19:38:16\n",
      "------- 40 24 48.77933530807495 2019-07-08 19:38:18\n",
      "------- 40 25 48.49177207946777 2019-07-08 19:38:20\n",
      "------- 40 26 48.21998596191406 2019-07-08 19:38:22\n",
      "------- 40 27 47.96348576545715 2019-07-08 19:38:25\n",
      "------- 40 28 47.721576595306395 2019-07-08 19:38:27\n",
      "------- 40 29 47.493544054031375 2019-07-08 19:38:30\n",
      "------- 40 30 47.27907528877258 2019-07-08 19:38:32\n",
      "------- 40 31 47.07705779075623 2019-07-08 19:38:34\n",
      "------- 40 32 46.88685908317566 2019-07-08 19:38:36\n",
      "------- 40 33 46.708566188812256 2019-07-08 19:38:38\n",
      "------- 40 34 46.540402841567996 2019-07-08 19:38:41\n",
      "------- 40 35 46.38154134750366 2019-07-08 19:38:43\n",
      "------- 40 36 46.23171825408936 2019-07-08 19:38:45\n",
      "------- 40 37 46.09049382209778 2019-07-08 19:38:48\n",
      "------- 40 38 45.956276655197144 2019-07-08 19:38:50\n",
      "------- 40 39 45.82958874702454 2019-07-08 19:38:52\n",
      "------- 40 40 45.70908918380737 2019-07-08 19:38:55\n",
      "------- 40 41 45.59447555541992 2019-07-08 19:38:57\n",
      "------- 40 42 45.48523859977722 2019-07-08 19:38:59\n",
      "------- 40 43 45.38146843910217 2019-07-08 19:39:02\n",
      "------- 40 44 45.28235054016113 2019-07-08 19:39:04\n",
      "------- 40 45 45.18760576248169 2019-07-08 19:39:07\n",
      "------- 40 46 45.09680252075195 2019-07-08 19:39:09\n",
      "------- 40 47 45.00955028533936 2019-07-08 19:39:11\n",
      "------- 40 48 44.925540924072266 2019-07-08 19:39:14\n",
      "------- 40 49 44.84442343711853 2019-07-08 19:39:16\n",
      "------- 40 0 366.98522148132326 2019-07-08 19:39:18\n",
      "------- 40 1 83.17793841362 2019-07-08 19:39:21\n",
      "------- 40 2 76.25543670654297 2019-07-08 19:39:24\n",
      "------- 40 3 71.79417452812194 2019-07-08 19:39:27\n",
      "------- 40 4 68.45883903503417 2019-07-08 19:39:31\n",
      "------- 40 5 65.71171274185181 2019-07-08 19:39:33\n",
      "------- 40 6 63.316284847259524 2019-07-08 19:39:36\n",
      "------- 40 7 61.170553636550906 2019-07-08 19:39:38\n",
      "------- 40 8 59.231163024902344 2019-07-08 19:39:40\n",
      "------- 40 9 57.480424308776854 2019-07-08 19:39:43\n",
      "------- 40 10 55.9216845035553 2019-07-08 19:39:46\n",
      "------- 40 11 54.55375394821167 2019-07-08 19:39:50\n",
      "------- 40 12 53.36973686218262 2019-07-08 19:39:52\n",
      "------- 40 13 52.35591201782226 2019-07-08 19:39:55\n",
      "------- 40 14 51.49265036582947 2019-07-08 19:39:57\n",
      "------- 40 15 50.75918006896973 2019-07-08 19:39:59\n",
      "------- 40 16 50.1327844619751 2019-07-08 19:40:02\n",
      "------- 40 17 49.592259359359744 2019-07-08 19:40:04\n",
      "------- 40 18 49.120812273025514 2019-07-08 19:40:07\n",
      "------- 40 19 48.706635570526124 2019-07-08 19:40:09\n",
      "------- 40 20 48.339604663848874 2019-07-08 19:40:11\n",
      "------- 40 21 48.01231608390808 2019-07-08 19:40:13\n",
      "------- 40 22 47.71816682815552 2019-07-08 19:40:15\n",
      "------- 40 23 47.452564573287965 2019-07-08 19:40:18\n",
      "------- 40 24 47.21153731346131 2019-07-08 19:40:20\n",
      "------- 40 25 46.990789079666136 2019-07-08 19:40:22\n",
      "------- 40 26 46.788384675979614 2019-07-08 19:40:25\n",
      "------- 40 27 46.6025806427002 2019-07-08 19:40:27\n",
      "------- 40 28 46.43089017868042 2019-07-08 19:40:29\n",
      "------- 40 29 46.272561979293826 2019-07-08 19:40:31\n",
      "------- 40 30 46.126154232025144 2019-07-08 19:40:33\n",
      "------- 40 31 45.9904408454895 2019-07-08 19:40:36\n",
      "------- 40 32 45.864485836029054 2019-07-08 19:40:38\n",
      "------- 40 33 45.7471305847168 2019-07-08 19:40:40\n",
      "------- 40 34 45.63761963844299 2019-07-08 19:40:42\n",
      "------- 40 35 45.535091972351076 2019-07-08 19:40:45\n",
      "------- 40 36 45.43910884857178 2019-07-08 19:40:47\n",
      "------- 40 37 45.34836163520813 2019-07-08 19:40:49\n",
      "------- 40 38 45.26379270553589 2019-07-08 19:40:51\n",
      "------- 40 39 45.18277382850647 2019-07-08 19:40:54\n",
      "------- 40 40 45.105965757369994 2019-07-08 19:40:56\n",
      "------- 40 41 45.03285179138184 2019-07-08 19:40:58\n",
      "------- 40 42 44.96276640892029 2019-07-08 19:41:00\n",
      "------- 40 43 44.89675259590149 2019-07-08 19:41:03\n",
      "------- 40 44 44.83328413963318 2019-07-08 19:41:05\n",
      "------- 40 45 44.772824382781984 2019-07-08 19:41:07\n",
      "------- 40 46 44.714373397827146 2019-07-08 19:41:09\n",
      "------- 40 47 44.6590075969696 2019-07-08 19:41:12\n",
      "------- 40 48 44.605440044403075 2019-07-08 19:41:14\n",
      "------- 40 49 44.55476541519165 2019-07-08 19:41:16\n"
     ]
    }
   ],
   "source": [
    "# defining training data\n",
    "# shift size indicate location of frames responding to matrix diagonal\n",
    "get_minimum = 0\n",
    "shift_size = 50\n",
    "low_resolution_samples = np.load(\"/Users/neda/HiCPlus_pytorch/data/divided-data/GM12878_primary/10kb_resolution/chr1-17(down16)(rep2).npy\", \"r\").astype(np.float32) * down_sample_ratio\n",
    "low_resolution_samples = np.expand_dims(low_resolution_samples, axis=1)\n",
    "high_resolution_samples = np.load(\"/Users/neda/HiCPlus_pytorch/data/divided-data/GM12878_primary/10kb_resolution/chr1-17.npy\", \"r\").astype(np.float32)\n",
    "high_resolution_samples = np.expand_dims(high_resolution_samples, axis=1)\n",
    "high_resolution_samples = high_resolution_samples[d_indices(shift_size)[0],:,:,:]\n",
    "low_resolution_samples = low_resolution_samples[d_indices(shift_size)[0],:,:,:]\n",
    "if get_minimum == 1:\n",
    "    high_resolution_samples = np.minimum(high_resolution_samples, HiC_max_value)\n",
    "    low_resolution_samples = np.minimum(low_resolution_samples, HiC_max_value)\n",
    "\n",
    "\n",
    "sample_size = high_resolution_samples.shape[-1]\n",
    "half_padding = int(model.half_padding)\n",
    "num_samples = low_resolution_samples.shape[0]\n",
    "\n",
    "\"\"\"\n",
    "lowres_set = torch.from_numpy(low_resolution_samples[39*256:40*256,])\n",
    "hires_set = torch.from_numpy(high_resolution_samples[39*256:40*256,])\n",
    "print(\"high and low loss: \", _loss(Variable(lowres_set), Variable(hires_set)).item())\n",
    "zero_data = torch.from_numpy(np.zeros((256,1,sample_size,sample_size), dtype = 'float32'))\n",
    "print(\"high and zero loss: \", _loss(Variable(zero_data), Variable(hires_set)).item())\n",
    "\"\"\"\n",
    "\n",
    "high_resolution_samples = high_resolution_samples[:,:,half_padding:(sample_size-half_padding),half_padding:(sample_size-half_padding)]\n",
    "lowres_set = data.TensorDataset(torch.from_numpy(low_resolution_samples), torch.from_numpy(np.zeros(low_resolution_samples.shape[0])))\n",
    "lowres_loader = torch.utils.data.DataLoader(lowres_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "hires_set = data.TensorDataset(torch.from_numpy(high_resolution_samples), torch.from_numpy(np.zeros(high_resolution_samples.shape[0])))\n",
    "hires_loader = torch.utils.data.DataLoader(hires_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for t in range(2):\n",
    "    # defining network\n",
    "    Net = model.Net(40, 28)\n",
    "    if use_gpu:\n",
    "        Net = Net.cuda()\n",
    "\n",
    "    optimizer = optim.SGD(Net.parameters(), lr = 0.00001)\n",
    "    _loss = nn.MSELoss()\n",
    "    Net.train()\n",
    "    running_loss = 0.0\n",
    "    losslist = []\n",
    "    for epoch in range(0, epochs):\n",
    "        # iterate over two lists and their indices using enumerate together with zip\n",
    "        # lowres_loader is list of batches\n",
    "        for i, (v1, v2) in enumerate(zip(lowres_loader, hires_loader)):\n",
    "            # probably it is for skipping last incomplete batch\n",
    "            if (i == len(lowres_loader) - 1):\n",
    "                continue\n",
    "\n",
    "\n",
    "            # v1 is list with length = 2. v1[0] is data tensor so with shape 256*1*40*40. v1[1] is vector of 256 zeros because pf line 85 but what's the reason?\n",
    "            _lowRes, _ = v1\n",
    "            _highRes, _ = v2\n",
    "            # print \"_lowres:\", _lowRes, \"\\n shape: \", _lowRes.shape\n",
    "\n",
    "            _lowRes = Variable(_lowRes)\n",
    "            _highRes = Variable(_highRes)\n",
    "\n",
    "\n",
    "            if use_gpu:\n",
    "                _lowRes = _lowRes.cuda()\n",
    "                _highRes = _highRes.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            y_prediction = Net(_lowRes)\n",
    "            loss = _loss(y_prediction, _highRes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(loss.item())\n",
    "            running_loss += loss.item()\n",
    "        print ('-------', i, epoch, running_loss/i, strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "        losslist.append(running_loss/i)\n",
    "        running_loss = 0.0\n",
    "    globals()[\"Net\" + str(t)] = Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_Net = Net0\n",
    "lowres_set = torch.from_numpy(low_resolution_samples)\n",
    "hires_set = torch.from_numpy(high_resolution_samples)\n",
    "loss_list = []\n",
    "for alpha in np.arange(0,1.01,0.01):\n",
    "    \n",
    "    for (temp_param, param1, param2) in zip(temp_Net.parameters(), Net0.parameters(), Net1.parameters()):\n",
    "        temp_param.data = (alpha * param1.data) + ((1 - alpha) * param2.data)\n",
    "    y_prediction = temp_Net(Variable(lowres_set))\n",
    "    loss_list.append(_loss(y_prediction, Variable(hires_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.001, 0.002, ..., 0.998, 0.999, 1.   ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.axis(np.arange(0,1.001,0.001))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of correlation based on location of frames responding to diagonal of matrix\n",
    "mean_int = {}\n",
    "for i in range(-200,201,25):\n",
    "    mean_int[i] = []\n",
    "    for j in d_indices(i)[0]:\n",
    "        mean_int[i].append(np.mean(low_resolution_samples[j,]))  \n",
    "corr_list = {}\n",
    "for i in range(-200,201,25):\n",
    "    corr_list[i] = []\n",
    "    for j in d_indices(i)[0]:\n",
    "        if np.sum(low_resolution_samples[j,:,:]) != 0 and np.sum(high_resolution_samples[j,:,:]) != 0:\n",
    "            corr_list[i].append(pearsonr(low_resolution_samples[j,0,:,:].flatten(),high_resolution_samples[j,0,:,:].flatten())[0]) \n",
    "mean_corr_list = [np.mean(corr_list[i]) for i in range(-200,201,25)]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(-200,201,25), mean_corr_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9636, 0.9208, 0.8555, 0.9137, 1.0335, 0.9685, 0.9639, 0.9570,\n",
      "           0.9550],\n",
      "          [0.8919, 0.9170, 0.9635, 0.9646, 0.9431, 0.9507, 1.0276, 0.9949,\n",
      "           0.8679],\n",
      "          [0.9759, 0.9762, 0.9202, 1.0667, 1.1664, 0.9842, 0.9624, 0.9654,\n",
      "           0.9873],\n",
      "          [0.9927, 0.9429, 1.1405, 1.1066, 1.3203, 1.1500, 1.0649, 1.0762,\n",
      "           1.0207],\n",
      "          [0.9871, 1.0559, 0.9653, 1.0166, 1.1858, 1.0775, 1.0898, 0.8916,\n",
      "           0.9334],\n",
      "          [0.9301, 0.9730, 0.9562, 0.9854, 1.0957, 1.0161, 1.0278, 0.9283,\n",
      "           0.8892],\n",
      "          [0.9084, 0.9565, 0.9525, 0.9693, 1.0521, 0.9755, 0.8908, 0.9872,\n",
      "           0.9022],\n",
      "          [0.9701, 0.9328, 1.0520, 0.9451, 0.9412, 0.9778, 0.9854, 0.8601,\n",
      "           0.9632],\n",
      "          [0.9036, 0.8899, 0.9923, 0.9370, 1.0664, 0.9955, 0.9547, 0.9384,\n",
      "           0.8331]]],\n",
      "\n",
      "\n",
      "        [[[0.9574, 0.9467, 1.0655, 1.0429, 1.1367, 0.9073, 1.0035, 1.0173,\n",
      "           0.9325],\n",
      "          [1.0506, 1.0344, 0.9583, 1.0991, 1.1006, 0.9129, 0.9897, 0.9751,\n",
      "           1.0275],\n",
      "          [0.9580, 0.9891, 1.0477, 0.9783, 0.9899, 1.0697, 0.9796, 1.0392,\n",
      "           0.9948],\n",
      "          [1.0353, 1.1279, 1.1619, 1.1709, 1.2124, 1.1168, 1.1544, 0.9597,\n",
      "           1.0846],\n",
      "          [0.9760, 0.9900, 0.9800, 0.9850, 1.1662, 1.0661, 1.1215, 0.9723,\n",
      "           1.0219],\n",
      "          [0.9452, 1.0200, 0.9325, 1.1519, 1.1076, 0.8768, 1.1235, 1.0396,\n",
      "           1.0032],\n",
      "          [1.0967, 1.0194, 1.0370, 1.0247, 1.1579, 0.9761, 0.9878, 1.0440,\n",
      "           1.0000],\n",
      "          [1.0419, 1.0793, 0.9727, 1.0235, 1.0877, 0.9415, 0.9600, 1.0108,\n",
      "           1.0165],\n",
      "          [1.0519, 1.0144, 1.0268, 1.0066, 1.0373, 0.9732, 1.0237, 0.9186,\n",
      "           0.9827]]],\n",
      "\n",
      "\n",
      "        [[[0.8938, 1.0143, 0.9159, 1.0583, 0.9530, 1.0542, 0.9305, 1.0791,\n",
      "           1.0479],\n",
      "          [0.9983, 0.9496, 0.9025, 1.0235, 0.9146, 0.9573, 1.0156, 1.0055,\n",
      "           0.9252],\n",
      "          [0.9230, 0.9935, 0.9012, 0.8970, 0.9423, 1.0671, 1.0649, 1.0378,\n",
      "           0.9482],\n",
      "          [1.0184, 1.0288, 0.9344, 0.9439, 0.9619, 0.9570, 0.9743, 1.0134,\n",
      "           0.9125],\n",
      "          [1.0551, 1.0637, 0.9954, 1.0399, 0.9871, 0.9156, 0.9172, 0.9399,\n",
      "           0.9500],\n",
      "          [1.0140, 0.9087, 1.0163, 0.9116, 0.8931, 1.0790, 0.9594, 0.8955,\n",
      "           1.0081],\n",
      "          [1.0572, 0.9126, 1.0278, 1.0008, 1.0632, 0.9703, 1.0801, 1.0820,\n",
      "           0.9068],\n",
      "          [0.9554, 1.0553, 1.0302, 0.9684, 0.8993, 0.9452, 1.0886, 0.9292,\n",
      "           1.0608],\n",
      "          [0.9272, 1.0999, 1.0475, 1.0442, 1.0396, 0.9684, 1.0389, 0.9535,\n",
      "           1.0686]]],\n",
      "\n",
      "\n",
      "        [[[0.9012, 0.9101, 1.0407, 1.0885, 1.0569, 0.9054, 0.9304, 0.9674,\n",
      "           0.9405],\n",
      "          [0.9260, 0.9802, 0.9042, 0.9943, 0.9207, 0.9520, 0.9092, 0.9111,\n",
      "           0.9331],\n",
      "          [1.0447, 1.0388, 1.0043, 1.0245, 0.9532, 1.0032, 0.9364, 1.0026,\n",
      "           1.0153],\n",
      "          [0.9964, 1.0170, 0.9919, 0.9938, 0.9179, 1.0255, 1.0109, 1.0888,\n",
      "           1.0706],\n",
      "          [1.0363, 0.9467, 1.0717, 1.0586, 0.9315, 1.0409, 1.0371, 0.9271,\n",
      "           0.9133],\n",
      "          [0.9764, 1.0377, 1.0151, 1.0131, 0.9225, 0.9009, 1.0546, 0.9903,\n",
      "           0.9394],\n",
      "          [1.0032, 1.0508, 1.0947, 1.1011, 1.0297, 0.8826, 1.0484, 1.0118,\n",
      "           0.9297],\n",
      "          [0.9051, 1.0247, 1.0674, 0.9361, 0.9779, 0.9659, 1.0217, 1.0450,\n",
      "           1.0770],\n",
      "          [0.9649, 1.0513, 1.0599, 1.0552, 1.0835, 1.0189, 1.0421, 0.9554,\n",
      "           0.8926]]],\n",
      "\n",
      "\n",
      "        [[[0.9330, 1.0367, 0.8843, 1.0088, 1.0259, 0.9565, 0.9244, 0.9284,\n",
      "           1.0550],\n",
      "          [1.0248, 1.0500, 0.9679, 0.9899, 1.0443, 1.0593, 1.0507, 0.9030,\n",
      "           1.0901],\n",
      "          [1.0509, 1.0024, 1.0686, 1.0618, 1.0056, 0.9972, 0.9973, 1.0599,\n",
      "           0.9432],\n",
      "          [0.9897, 0.9666, 1.0857, 0.8779, 1.1098, 0.9124, 0.9276, 1.0167,\n",
      "           1.0107],\n",
      "          [1.0576, 1.0419, 1.0910, 0.8943, 0.9860, 1.0346, 1.0354, 1.0583,\n",
      "           1.0427],\n",
      "          [0.9560, 0.8986, 0.9909, 0.9324, 0.9382, 0.9723, 0.9069, 0.9450,\n",
      "           0.9767],\n",
      "          [1.0594, 1.0734, 0.9493, 1.0604, 0.8928, 0.9736, 1.0015, 0.9376,\n",
      "           0.9474],\n",
      "          [0.9998, 0.9636, 0.9012, 0.9160, 0.9143, 0.9163, 0.9620, 1.0321,\n",
      "           1.0034],\n",
      "          [0.9112, 1.0207, 0.9015, 0.9023, 1.0468, 1.0786, 1.0229, 0.9490,\n",
      "           1.0933]]],\n",
      "\n",
      "\n",
      "        [[[1.0063, 0.9494, 1.1193, 1.0746, 0.9455, 1.0833, 0.9632, 1.0325,\n",
      "           1.0235],\n",
      "          [0.9966, 0.9710, 0.9514, 1.0217, 1.0515, 0.9173, 1.0747, 1.0104,\n",
      "           1.0137],\n",
      "          [0.9859, 1.0305, 0.9492, 1.1004, 1.0140, 0.9645, 1.0524, 0.9589,\n",
      "           1.0716],\n",
      "          [0.8136, 1.0163, 1.0778, 0.9700, 0.8781, 1.0125, 0.9794, 1.0601,\n",
      "           0.9511],\n",
      "          [0.9741, 1.0033, 0.9888, 1.0878, 0.9190, 1.0299, 1.0778, 0.9809,\n",
      "           0.9345],\n",
      "          [1.0131, 0.9641, 0.9594, 1.0385, 1.1459, 1.0106, 1.1299, 1.0707,\n",
      "           1.1416],\n",
      "          [1.1054, 1.1259, 1.0243, 1.0669, 0.9371, 0.9834, 0.9545, 0.9551,\n",
      "           0.9441],\n",
      "          [1.0178, 1.0379, 1.0954, 1.0075, 1.0275, 1.0527, 0.9486, 1.0405,\n",
      "           1.0254],\n",
      "          [1.0344, 1.0647, 1.0861, 1.0002, 0.9067, 0.9433, 0.9203, 0.9660,\n",
      "           0.9766]]],\n",
      "\n",
      "\n",
      "        [[[1.0487, 1.0666, 1.0595, 0.9390, 0.9455, 0.9761, 1.0916, 0.9532,\n",
      "           1.0885],\n",
      "          [1.0795, 0.9461, 0.9410, 1.0453, 1.1158, 1.0142, 1.0935, 0.9727,\n",
      "           1.0908],\n",
      "          [0.9651, 1.0457, 0.9877, 0.9678, 0.9241, 0.9527, 0.8975, 1.0516,\n",
      "           0.9864],\n",
      "          [1.0025, 1.0023, 1.1175, 0.9386, 0.9833, 0.9538, 0.9058, 1.1202,\n",
      "           1.0272],\n",
      "          [0.9175, 0.9538, 1.0416, 1.0300, 1.0880, 0.9349, 0.9694, 1.0639,\n",
      "           0.9855],\n",
      "          [1.0460, 1.0568, 1.0086, 0.9035, 0.9691, 1.1333, 1.0680, 1.0644,\n",
      "           0.9222],\n",
      "          [1.0457, 1.0037, 0.9602, 1.0704, 0.9644, 0.9315, 1.0607, 1.0242,\n",
      "           0.9676],\n",
      "          [0.9661, 1.0067, 1.0008, 1.0927, 1.0823, 1.0595, 1.0085, 1.0101,\n",
      "           1.0244],\n",
      "          [1.1034, 1.0850, 0.8793, 0.9311, 1.0028, 0.9219, 0.9689, 0.9766,\n",
      "           0.9850]]],\n",
      "\n",
      "\n",
      "        [[[1.0647, 0.9962, 0.8857, 0.9325, 0.9523, 0.9904, 0.9080, 1.0777,\n",
      "           1.0337],\n",
      "          [1.0666, 1.0589, 0.9925, 0.9169, 0.9153, 1.0438, 1.0678, 1.0707,\n",
      "           0.9704],\n",
      "          [1.0798, 0.9211, 1.0609, 0.9490, 1.0280, 0.9379, 1.0494, 1.0699,\n",
      "           0.9732],\n",
      "          [0.9964, 1.0413, 1.1024, 1.0756, 0.9212, 1.0488, 1.0443, 1.0771,\n",
      "           0.9533],\n",
      "          [0.9477, 1.0726, 1.0078, 1.0582, 0.9064, 0.9135, 0.9524, 0.9522,\n",
      "           1.0634],\n",
      "          [0.9840, 1.1004, 1.0309, 0.9532, 0.9912, 0.9929, 0.9450, 0.8927,\n",
      "           1.0604],\n",
      "          [1.0374, 1.0540, 1.0224, 1.0504, 1.0353, 0.9805, 0.8973, 1.0492,\n",
      "           0.9031],\n",
      "          [0.9590, 0.9487, 1.0594, 0.8766, 1.0398, 1.0076, 0.8869, 0.9974,\n",
      "           0.8768],\n",
      "          [1.0942, 0.9747, 0.8908, 0.8790, 1.0398, 0.9113, 0.9494, 0.9073,\n",
      "           0.9098]]]])\n",
      "tensor([1.0506, 1.0420, 0.9322, 0.9319, 0.9739, 1.0184, 1.0743, 1.1051])\n",
      "tensor([[[[0.9330]],\n",
      "\n",
      "         [[0.8332]],\n",
      "\n",
      "         [[1.1501]],\n",
      "\n",
      "         [[1.0403]],\n",
      "\n",
      "         [[0.8506]],\n",
      "\n",
      "         [[1.3130]],\n",
      "\n",
      "         [[0.7109]],\n",
      "\n",
      "         [[1.1420]]],\n",
      "\n",
      "\n",
      "        [[[1.2644]],\n",
      "\n",
      "         [[1.3300]],\n",
      "\n",
      "         [[1.0296]],\n",
      "\n",
      "         [[0.7944]],\n",
      "\n",
      "         [[1.0559]],\n",
      "\n",
      "         [[1.2330]],\n",
      "\n",
      "         [[1.1752]],\n",
      "\n",
      "         [[0.6771]]],\n",
      "\n",
      "\n",
      "        [[[1.4100]],\n",
      "\n",
      "         [[1.0006]],\n",
      "\n",
      "         [[1.0001]],\n",
      "\n",
      "         [[0.7787]],\n",
      "\n",
      "         [[0.7481]],\n",
      "\n",
      "         [[0.9165]],\n",
      "\n",
      "         [[0.9876]],\n",
      "\n",
      "         [[1.0553]]],\n",
      "\n",
      "\n",
      "        [[[1.3517]],\n",
      "\n",
      "         [[1.3155]],\n",
      "\n",
      "         [[0.6733]],\n",
      "\n",
      "         [[0.8232]],\n",
      "\n",
      "         [[0.6806]],\n",
      "\n",
      "         [[0.6900]],\n",
      "\n",
      "         [[1.1107]],\n",
      "\n",
      "         [[0.7567]]],\n",
      "\n",
      "\n",
      "        [[[0.7292]],\n",
      "\n",
      "         [[1.2510]],\n",
      "\n",
      "         [[0.6735]],\n",
      "\n",
      "         [[1.2418]],\n",
      "\n",
      "         [[0.9728]],\n",
      "\n",
      "         [[1.1261]],\n",
      "\n",
      "         [[0.8425]],\n",
      "\n",
      "         [[0.9036]]],\n",
      "\n",
      "\n",
      "        [[[1.1626]],\n",
      "\n",
      "         [[1.1108]],\n",
      "\n",
      "         [[0.7910]],\n",
      "\n",
      "         [[1.1704]],\n",
      "\n",
      "         [[0.6832]],\n",
      "\n",
      "         [[0.8041]],\n",
      "\n",
      "         [[0.9572]],\n",
      "\n",
      "         [[0.7716]]],\n",
      "\n",
      "\n",
      "        [[[1.1185]],\n",
      "\n",
      "         [[1.2622]],\n",
      "\n",
      "         [[1.1783]],\n",
      "\n",
      "         [[0.9425]],\n",
      "\n",
      "         [[0.9348]],\n",
      "\n",
      "         [[0.7235]],\n",
      "\n",
      "         [[0.7367]],\n",
      "\n",
      "         [[1.0086]]],\n",
      "\n",
      "\n",
      "        [[[1.3498]],\n",
      "\n",
      "         [[0.8601]],\n",
      "\n",
      "         [[0.9999]],\n",
      "\n",
      "         [[0.6469]],\n",
      "\n",
      "         [[1.0502]],\n",
      "\n",
      "         [[0.6610]],\n",
      "\n",
      "         [[1.2941]],\n",
      "\n",
      "         [[0.7473]]]])\n",
      "tensor([1.0405, 0.6750, 0.7502, 0.9273, 0.9974, 0.7114, 0.7793, 0.9721])\n",
      "tensor([[[[1.0073, 1.0160, 0.9421, 0.9379, 1.0003],\n",
      "          [1.0021, 0.9490, 1.0300, 0.9808, 1.0497],\n",
      "          [0.9689, 1.0401, 0.9551, 1.0345, 0.9394],\n",
      "          [1.0194, 1.0009, 0.9787, 0.9477, 0.9827],\n",
      "          [0.9905, 0.9311, 0.9683, 0.9514, 0.9218]],\n",
      "\n",
      "         [[1.0888, 0.9791, 1.0201, 0.9648, 0.9768],\n",
      "          [0.9916, 0.9885, 1.1224, 1.0697, 1.0586],\n",
      "          [1.0252, 0.9674, 1.0504, 0.9870, 0.9612],\n",
      "          [0.9648, 1.0310, 1.1205, 1.0666, 1.1019],\n",
      "          [0.9410, 0.9300, 1.0007, 1.0691, 0.9695]],\n",
      "\n",
      "         [[0.9107, 0.9897, 1.0081, 1.0204, 0.9678],\n",
      "          [0.9862, 1.0334, 1.0447, 0.9357, 0.9282],\n",
      "          [0.9728, 1.0377, 1.1682, 1.0155, 0.9286],\n",
      "          [1.0328, 1.0750, 1.1928, 0.9994, 1.0461],\n",
      "          [1.0293, 0.9557, 1.1019, 0.9236, 0.9535]],\n",
      "\n",
      "         [[0.9079, 0.9747, 1.0319, 0.9795, 1.0034],\n",
      "          [1.0290, 0.9529, 1.0020, 0.9818, 0.9536],\n",
      "          [1.0033, 1.0489, 1.1302, 0.9828, 1.0058],\n",
      "          [1.1100, 1.0908, 1.2140, 1.0965, 0.9923],\n",
      "          [1.0219, 1.0055, 1.0308, 0.9468, 0.9102]],\n",
      "\n",
      "         [[0.9962, 0.9960, 1.0736, 1.1076, 1.0910],\n",
      "          [0.9964, 0.9363, 0.9774, 1.1594, 1.0382],\n",
      "          [1.0026, 0.9848, 0.9662, 1.0649, 1.0007],\n",
      "          [1.0045, 0.9951, 0.8635, 1.0298, 1.0175],\n",
      "          [1.0602, 0.9903, 0.9814, 1.0429, 1.0160]],\n",
      "\n",
      "         [[0.9625, 0.9250, 1.0328, 0.9963, 1.0629],\n",
      "          [0.9331, 0.9936, 1.0039, 0.9227, 0.9639],\n",
      "          [1.0729, 1.0132, 1.0513, 1.0208, 1.0280],\n",
      "          [1.0759, 1.0752, 1.0943, 1.0736, 0.9429],\n",
      "          [0.9642, 1.0293, 1.0360, 0.9988, 0.9700]],\n",
      "\n",
      "         [[1.0225, 0.9869, 1.0712, 1.0043, 0.9705],\n",
      "          [0.9680, 0.9204, 1.0275, 0.9935, 1.0119],\n",
      "          [1.0277, 1.0157, 1.0941, 0.9555, 0.9755],\n",
      "          [1.0559, 1.0450, 1.1682, 1.0456, 1.0220],\n",
      "          [1.0081, 0.9910, 1.0549, 1.0004, 1.0592]],\n",
      "\n",
      "         [[0.9673, 0.9678, 1.0041, 0.9280, 1.0465],\n",
      "          [0.9655, 1.0302, 0.9322, 0.9442, 1.0714],\n",
      "          [0.9406, 0.9914, 1.0464, 0.9468, 0.9727],\n",
      "          [0.9585, 1.0580, 1.1117, 1.0472, 0.9964],\n",
      "          [1.0240, 0.9620, 1.0441, 0.9543, 1.0243]]]])\n",
      "tensor([0.9708])\n"
     ]
    }
   ],
   "source": [
    "for param in temp_Net.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
